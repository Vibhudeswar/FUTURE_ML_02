# ğŸ« AI Support Ticket Classifier

> Automatically classify IT service tickets into **8 categories** and predict **priority levels** for customer support tickets using NLP and Machine Learning â€” built end-to-end in Python.

<br>

![Python](https://img.shields.io/badge/Python-3.10-3776AB?style=for-the-badge&logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-ML-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![NLTK](https://img.shields.io/badge/NLTK-NLP-4B8BBE?style=for-the-badge)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Colab](https://img.shields.io/badge/Google-Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)

---

## ğŸ“Œ Project Overview

This project builds a **complete machine learning pipeline** from raw support tickets to a live, deployable classifier. It covers:

- ğŸ·ï¸ **IT Ticket Classification** â€” 8 categories (Hardware, HR Support, Access, Storage, etc.)
- ğŸš¨ **Priority Prediction** â€” High / Medium / Low for customer support tickets
- ğŸ§¹ **NLTK Preprocessing Pipeline** â€” tokenize â†’ filter â†’ lemmatize â†’ TF-IDF
- ğŸ¤– **4 ML models trained and benchmarked** side by side
- ğŸ“Š **Executive Dashboard** with KPI cards, confusion matrices, and CV results
- ğŸ’¾ **Exported models** (pickle) + downloadable ZIP of all outputs

**Built for:** Future Interns â€” Machine Learning Task 2 (2026)

---

## ğŸ“Š Results

| Metric | Value |
|---|---|
| ğŸ“‚ Total Tickets Processed | **56,306** |
| ğŸ·ï¸ IT Category â€” Weighted F1 | **84.5%** |
| ğŸ” 5-Fold Cross-Validation F1 | **85.1% Â± 0.2%** |
| âš¡ Inference Time | **< 5ms per ticket** |
| ğŸ† Best Model | **Logistic Regression** |

### Model Comparison â€” IT Category Classification

| Model | Accuracy | Precision | Recall | Weighted F1 |
|---|---|---|---|---|
| â­ Logistic Regression | 0.8450 | 0.8455 | 0.8450 | **0.8451** |
| Linear SVM | 0.8423 | 0.8430 | 0.8423 | 0.8424 |
| Random Forest | 0.8342 | 0.8350 | 0.8342 | 0.8343 |
| Naive Bayes | 0.7713 | 0.7719 | 0.7713 | 0.7712 |

---

## ğŸ—‚ï¸ Repository Structure

```
support-ticket-classifier/
â”‚
â”œâ”€â”€ ğŸ““ support_ticket_classifier.ipynb   â† Full notebook (13 steps)
â”‚
â”œâ”€â”€ ğŸ“Š outputs/
â”‚   â”œâ”€â”€ 01_eda_overview.png              â† Dataset EDA charts
â”‚   â”œâ”€â”€ 02_wordclouds.png                â† Top keywords per category
â”‚   â”œâ”€â”€ 03_confusion_matrices.png        â† IT + CS evaluation heatmaps
â”‚   â”œâ”€â”€ 04_model_comparison.png          â† 4-model benchmark bar chart
â”‚   â”œâ”€â”€ 05_cross_validation.png          â† 5-fold CV line chart
â”‚   â”œâ”€â”€ 06_feature_importance.png        â† TF-IDF top terms per class
â”‚   â”œâ”€â”€ 07_executive_dashboard.png       â† Full KPI dashboard
â”‚   â””â”€â”€ 08_nltk_pipeline.png             â† Preprocessing flow diagram
â”‚
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ requirements.txt
â””â”€â”€ ğŸ“„ .gitignore
```

> âš ï¸ **Datasets not included** (too large for GitHub). See the [Datasets](#-datasets) section below.

---

## ğŸ—ƒï¸ Datasets

Two datasets are used. Upload them when running in Google Colab:

| Dataset | Rows | Cols | Target |
|---|---|---|---|
| `all_tickets_processed_improved_v3.csv` | 47,837 | 2 | `Topic_group` â€” 8 classes |
| `customer_support_tickets.csv` | 8,469 | 17 | `Ticket Priority` â€” 3 classes |

**IT Ticket Categories (8):**
Hardware Â· HR Support Â· Access Â· Miscellaneous Â· Storage Â· Purchase Â· Internal Project Â· Administrative Rights

**Priority Levels (3):**
ğŸ”´ High Â· ğŸŸ¡ Medium Â· ğŸŸ¢ Low *(Critical merged into High)*

---

## ğŸ”§ Tech Stack

| Layer | Library / Tool |
|---|---|
| Language | Python 3.10 |
| NLP | `nltk` â€” word_tokenize, stopwords, WordNetLemmatizer, PorterStemmer |
| Features | `TfidfVectorizer` â€” 15K features, ngram(1,3), sublinear TF |
| Models | LogisticRegression, LinearSVC, RandomForestClassifier, MultinomialNB |
| Evaluation | classification_report, confusion_matrix, StratifiedKFold, cross_val_score |
| Visualization | matplotlib, matplotlib.gridspec, seaborn, wordcloud |
| Environment | Jupyter Notebook / Google Colab |
| Export | pickle, zipfile |

---

## ğŸ§¹ NLTK Preprocessing Pipeline (Step 5)

```
Raw Ticket Text
      â”‚
      â–¼   re.sub() â€” remove {placeholders}, URLs, emails, numbers
      â”‚
      â–¼   word_tokenize()         â† NLTK punkt tokenizer
      â”‚
      â–¼   Filter stopwords        â† NLTK corpus + 23 domain-specific words
      â”‚                              (ticket, support, regards, dear, ...)
      â–¼   WordNetLemmatizer()     â† Lemmatization (default mode)
      â”‚   PorterStemmer()         â† Optional stemming mode
      â”‚
      â–¼   TfidfVectorizer()       â† 15,000 features, ngram(1,3), sublinear_tf
      â”‚
      â–¼   ML Model â†’ Prediction + Confidence Score
```

**Real example from the notebook:**

```python
Input     : "My laptop keyboard stopped working after the Windows update yesterday."
Tokens    : ['my', 'laptop', 'keyboard', 'stopped', 'working', 'after', ...]
Filtered  : ['laptop', 'keyboard', 'stopped', 'working', 'windows', 'update']
Lemmatized: ['laptop', 'keyboard', 'stop', 'work', 'window', 'update']
TF-IDF    : [0.0, 0.82, 0.71, 0.0, 0.63, ...]

â†’ Prediction: ğŸ–¥ï¸  Hardware  (confidence: 87.3%)
```

---

## ğŸ““ Notebook Walkthrough â€” 13 Steps

| Step | Title | What It Does |
|---|---|---|
| 1 | Install & Import Libraries | `pip install nltk wordcloud`; all sklearn/matplotlib imports; dark theme palette |
| 2 | Load Datasets | Upload both CSVs via Colab file picker; auto-detects IT vs CS dataset by column names |
| 3 | Exploratory Data Analysis | Category bar chart, ticket-type pie, priority bars, word-length histogram |
| 4 | Data Quality Audit | Cross-tab analysis; discovers CS text fields carry no signal (synthetically generated) |
| 5 | Text Preprocessing with NLTK | `clean_text()` â€” regex cleaning â†’ word_tokenize â†’ stopword filter â†’ lemmatize |
| 6 | Word Clouds | Per-category word clouds showing most discriminative terms |
| 7 | Model Training â€” IT Category | TF-IDF + 4 models; stratified 80/20 split; full accuracy/F1 score table |
| 8 | Model Training â€” CS Priority | TF-IDF + one-hot metadata â†’ combined sparse feature matrix; 4 models |
| 9 | Evaluation | `classification_report` for both tasks; styled confusion matrices with % annotations |
| 9b | Cross-Validation | 5-fold `StratifiedKFold` on a `Pipeline`; fill-between confidence bands plotted |
| 10 | Feature Importance | Logistic Regression coefficients â†’ top 10 TF-IDF terms per category (2Ã—4 grid) |
| 11 | Live Inference | `classify_it_ticket()` + `classify_cs_priority()` with top-3 confidence scores |
| 12 | Executive Dashboard | 4 KPI cards + category bars + CV plot + per-class F1 heatmap in one figure |
| 13 | Save & Download | Pickle all models + vectorizers + label encoders â†’ ZIP download |

---

## ğŸš€ Quick Start

### Option A â€” Google Colab (Recommended)

1. Upload `support_ticket_classifier.ipynb` to [Google Colab](https://colab.research.google.com/)
2. Run **Step 1** to install dependencies
3. Run **Step 2** â€” upload both CSV files when prompted
4. Run all remaining steps top-to-bottom

### Option B â€” Local Jupyter

```bash
# 1. Clone the repo
git clone https://github.com/YOUR_USERNAME/support-ticket-classifier.git
cd support-ticket-classifier

# 2. Install dependencies
pip install -r requirements.txt

# 3. Launch notebook
jupyter notebook support_ticket_classifier.ipynb
```

> **Local tip:** Replace the Colab upload cells (Steps 2â€“3) with:
> ```python
> df_it = pd.read_csv('all_tickets_processed_improved_v3.csv')
> df_cs = pd.read_csv('customer_support_tickets.csv')
> ```

---

## ğŸ” Live Inference (Step 11)

Two ready-to-use functions are provided:

```python
# Classify an IT service ticket â†’ category + confidence
result = classify_it_ticket(
    "My laptop screen is flickering after a Windows update."
)
# Returns:
# {
#   'category'  : 'Hardware',
#   'confidence': '89.2%',
#   'top3'      : [('Hardware','89.2%'), ('Access','5.1%'), ('Misc','3.4%')]
# }

# Predict priority for a customer support ticket
result = classify_cs_priority(
    "My account was charged twice. I need an immediate refund.",
    ticket_type='Billing inquiry',
    ticket_subject='Payment issue',
    channel='Chat',
    product='Apple AirPods'
)
# Returns:
# {
#   'priority'  : 'High',
#   'badge'     : 'ğŸ”´ HIGH',
#   'confidence': '91.4%'
# }
```

---

## ğŸ“ˆ Output Charts

| File | Description |
|---|---|
| `01_eda_overview.png` | IT category horizontal bars Â· CS ticket-type pie Â· priority bar Â· ticket-length histogram |
| `02_wordclouds.png` | Top keywords for each of the 8 IT categories |
| `03_confusion_matrices.png` | Side-by-side heatmaps â€” IT (8Ã—8) + CS priority (3Ã—3) with count and % annotations |
| `04_model_comparison.png` | Grouped bar chart: Accuracy + Weighted F1 for all 4 models across both tasks |
| `05_cross_validation.png` | 5-fold CV scores per fold with mean line and confidence band |
| `06_feature_importance.png` | Top 10 LR coefficients per IT category shown in a 2Ã—4 subplot grid |
| `07_executive_dashboard.png` | Full KPI dashboard: 4 cards + all charts in one publication-ready figure |
| `08_nltk_pipeline.png` | Visual step-by-step diagram of the NLTK preprocessing pipeline |

---

## ğŸ’¾ Exported Files (Step 13)

```
saved_models/
â”œâ”€â”€ tfidf_it.pkl         â† TF-IDF vectorizer for IT tickets   (15K features)
â”œâ”€â”€ tfidf_cs.pkl         â† TF-IDF vectorizer for CS tickets    (8K features)
â”œâ”€â”€ model_it_cat.pkl     â† Best IT category classifier (Logistic Regression)
â”œâ”€â”€ model_cs_pri.pkl     â† Best CS priority classifier
â”œâ”€â”€ le_it.pkl            â† LabelEncoder for 8 IT categories
â””â”€â”€ le_pri.pkl           â† LabelEncoder for 3 priority levels

model_metrics.csv              â† All model scores in one CSV
ticket_classifier_outputs.zip  â† Everything above bundled for download
```

---

## ğŸ“¦ Requirements

```
numpy
pandas
matplotlib
seaborn
scikit-learn
nltk
wordcloud
scipy
```

Install with:

```bash
pip install -r requirements.txt
```

---

## ğŸ¤ Connect

Built with â¤ï¸ as part of the **Future Interns ML Internship â€” 2026**

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin)](https://linkedin.com/in/YOUR_PROFILE)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-181717?style=for-the-badge&logo=github)](https://github.com/YOUR_USERNAME)

---

## ğŸ“„ License

This project is open-source under the [MIT License](LICENSE).
